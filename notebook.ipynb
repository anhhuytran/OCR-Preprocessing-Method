{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from asrtoolkit import cer, wer\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2text(fileName, process = True):\n",
    "    img = cv2.imread(fileName, cv2.IMREAD_GRAYSCALE)\n",
    "    if process:\n",
    "        img = processImg(img)\n",
    "\n",
    "    return pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImg(img):\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCER(begin, end, path = 'test_images/captured/grayscale/..._CC_GRAY.jpg'):\n",
    "    res = []\n",
    "    for i in range(begin, end + 1):\n",
    "        fileName = str(i).zfill(3)\n",
    "        imageName = path.replace('...', fileName)\n",
    "        truthName = f'test_images/truth/{fileName}_OCR_ASCII_TEXT_GT.txt'\n",
    "\n",
    "        text = img2text(imageName)\n",
    "        with open(truthName, 'r') as f:\n",
    "            groundTruth = f.read()\n",
    "\n",
    "        val = cer(groundTruth, text)\n",
    "        res.append(val)\n",
    "        print(f'{fileName}: {val}')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "011: 44.52662721893491\n",
      "012: 78.02844531633154\n",
      "013: 154.44015444015443\n",
      "92.3317423251403\n"
     ]
    }
   ],
   "source": [
    "CERs = calcCER(11,13)\n",
    "print(mean(CERs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "011: 3.4023668639053253\n",
      "012: 20.20598332515939\n",
      "013: 136.87258687258688\n",
      "53.493645687217196\n"
     ]
    }
   ],
   "source": [
    "#calc binary file\n",
    "CERs = calcCER(11,13, 'test_images/captured/binarized/..._SCANNED_BIN_GT.png')\n",
    "print(mean(CERs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CHAPTER 3. CONCEPT LEARNING FROM WEB VIDEO\n\nMAP on validation set MAP on test set\n\n10\n\n08\n\n  \n\n23 FF 0.40 =\n\n& 2 S 0.35\n\ngz rot os 3\n\n° as 0.30 0.90\nos\n\na ass 028 0.25\nC ™ 0.20 0.20\n\nQ\n\no\n\n00 02 04 O86 O8 10 00 02 04 O06 O8 1.0\nwelght{SIFT) welght(SIFT)\n\n{a) (b)\n\nFigure 3.11: The concept detection performance (MAP) of TubeTagger plotted against\nthe weights for SIFT+SVM and Motion. Similar performance can be observed on the\nvalidation set (a) and on the test set (b), which indicates that feature weights can be\nlearned reliably.\n\nmance for visual word approaches for the domain of news video [vdSGS08] — ob-\nviously, similar observations hold for web video content. Exceptions are some con-\ncepts for which the color+texture pipeline improves performance strongly. These\nare mostly sports concepts, like “golf”, “basketball”, and “swimming”, for which\ncolor is obviously a strong clue. Also, sports-related concepts reach the highest\noverall performance, obviously because they come with a static global frame layout\nand low intra-concept variance (an average precision of up to 86.5% [“soccer”] is\nreached). In contrast to this, for the most difficult concept “beach” only 15.9%\nare achieved. A closer look at this concept is taken in Figure 3.10, including the\n5 “beach” videos with the lowest scores (false negatives). Only one of these false\nnegatives is visually related to the tag “beach”, while the others show videoblogs\ntagged with “beach” and nightly parties in Miami Beach. Obviously, the reason\nfor system failure in these cases is that the relation between a clip and its tags is\nsubtle and extraordinarily difficult to infer from the visual content only.\n\nTo test whether the feature weights wy, ...,w4 (Equation (3.1)) can be learned\nreliably on the validation set, performance is plotted in Figure 3.11, both for the\nvalidation set and the test set (note that DCT features were left out, and that the\nweight of the C+T pipeline adds up to 1). Though the performance is lower on the\ntest set, a similar behavior of tagging performance can be observed for validation\nand testing, which indicates that feature weights can be learned reliably.\n\n49\n\f\n"
     ]
    }
   ],
   "source": [
    "#print text of one image\n",
    "fileName = '012'\n",
    "imageName = f'test_images/captured/grayscale/{fileName}_CC_GRAY.jpg'\n",
    "binaryName = f'test_images/captured/binarized/{fileName}_SCANNED_BIN_GT.png'\n",
    "\n",
    "print(img2text(binaryName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "edged = cv2.Canny(img, 100, 200)\n",
    "\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:10]\n",
    "screenCnt = None\n",
    "\n",
    "# loop over our contours\n",
    "for c in cnts:\n",
    "\t# approximate the contour\n",
    "\tperi = cv2.arcLength(c, True)\n",
    "\tapprox = cv2.approxPolyDP(c, 0.015 * peri, True)\n",
    "\t# if our approximated contour has four points, then\n",
    "\t# we can assume that we have found our screen\n",
    "\tif len(approx) == 4:\n",
    "\t\tscreenCnt = approx\n",
    "\t\tbreak\n",
    "\n",
    "cv2.drawContours(edged, [screenCnt], -1, (0, 255, 0), 3) \n",
    "\n",
    "cv2.imwrite('origin.jpg', edged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}